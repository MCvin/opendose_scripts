{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute SAFs from DoseByRegions.txt and fill the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(user, password, db, host='localhost'):\n",
    "    '''Returns a connection and a metadata object'''\n",
    "    # We connect with the help of the URL\n",
    "    # postgresql://postgres:postgres@localhost:5432/opendose\n",
    "    url = '{}://{}:{}@{}/{}'\n",
    "    url = url.format('postgresql', user, password, host, db)\n",
    "\n",
    "    # The return value of create_engine() is our connection object\n",
    "    con = sqlalchemy.create_engine(url)#, client_encoding='utf8')\n",
    "    # We then bind the connection to MetaData()\n",
    "    meta = sqlalchemy.MetaData(bind=con) #, reflect=True)\n",
    "    meta.reflect()\n",
    "\n",
    "    return con, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_single_sql_query(df, col_id, table, con):\n",
    "    '''Returns index of a column from a sql query'''\n",
    "    res = []\n",
    "    # allow only single df line query\n",
    "    if len(df.index) != 1:\n",
    "        return None\n",
    "    # compose the sql query from the df dataframe\n",
    "    sql='SELECT '+col_id+' FROM '+table+' WHERE '\n",
    "    for col in df.columns:\n",
    "        if col is not col_id:\n",
    "            sql += col+\"='\"+df[col].iloc[0]+\"' AND \"\n",
    "    sql = sql[:-5]+';'\n",
    "    # execute the sql query on the database table\n",
    "    for line in con.execute(sql):\n",
    "        res.append(line[col_id])\n",
    "    # return the index corresponding to the sql query (if one)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the OpenDose database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://postgres:***@localhost/opendose)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con, meta = connect(user='postgres', password='CRCT_eq15', db='opendose')\n",
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read tables from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provenance_id</th>\n",
       "      <th>provider</th>\n",
       "      <th>code</th>\n",
       "      <th>version</th>\n",
       "      <th>contact</th>\n",
       "      <th>email</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NPL</td>\n",
       "      <td>EGS++</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ana Denis-Bacelar</td>\n",
       "      <td>ana.denisbacelar@npl.co.uk</td>\n",
       "      <td>2017-09-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SCK.CEN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.7</td>\n",
       "      <td>Jérémie Dabin</td>\n",
       "      <td>jeremie.dabin@sckcen.be</td>\n",
       "      <td>2017-10-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2017-09-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2018-07-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-06-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-12-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-07-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-08-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>SGH</td>\n",
       "      <td>GATE</td>\n",
       "      <td>7.2</td>\n",
       "      <td>Erin McKay</td>\n",
       "      <td>erin@computerhead.com.au</td>\n",
       "      <td>2018-09-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>CRCT</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Maxime Chauvin</td>\n",
       "      <td>maxime.chauvin@inserm.fr</td>\n",
       "      <td>2017-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>IEO-CNAO</td>\n",
       "      <td>FLUKA</td>\n",
       "      <td>2011</td>\n",
       "      <td>Francesca Botta</td>\n",
       "      <td>francesca.botta@ieo.it</td>\n",
       "      <td>2018-12-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>CRUK</td>\n",
       "      <td>PENELOPE</td>\n",
       "      <td>2014</td>\n",
       "      <td>Nadia Falzone</td>\n",
       "      <td>nadia.falzone@oncology.ox.ac.uk</td>\n",
       "      <td>2017-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>CRCT</td>\n",
       "      <td>Geant4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Alex Vergara Gil</td>\n",
       "      <td>alex.vergara-gil@inserm.fr</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>NPL</td>\n",
       "      <td>EGS++</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ana Denis-Bacelar</td>\n",
       "      <td>ana.denisbacelar@npl.co.uk</td>\n",
       "      <td>2019-04-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>47</td>\n",
       "      <td>CRUK</td>\n",
       "      <td>PENELOPE</td>\n",
       "      <td>2014</td>\n",
       "      <td>Nadia Falzone</td>\n",
       "      <td>nadia.falzone@oncology.ox.ac.uk</td>\n",
       "      <td>2019-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>48</td>\n",
       "      <td>CRUK</td>\n",
       "      <td>PENELOPE</td>\n",
       "      <td>2014</td>\n",
       "      <td>Nadia Falzone</td>\n",
       "      <td>nadia.falzone@oncology.ox.ac.uk</td>\n",
       "      <td>2019-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>49</td>\n",
       "      <td>NPL</td>\n",
       "      <td>EGS++</td>\n",
       "      <td>2018</td>\n",
       "      <td>Ana Denis-Bacelar</td>\n",
       "      <td>ana.denisbacelar@npl.co.uk</td>\n",
       "      <td>2019-10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>CRCT</td>\n",
       "      <td>Geant4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Maxime Chauvin</td>\n",
       "      <td>maxime.chauvin@inserm.fr</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>52</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>53</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>56</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>54</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>55</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>SCK.CEN</td>\n",
       "      <td>PHITS</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Jérémie Dabin</td>\n",
       "      <td>jeremie.dabin@sckcen.be</td>\n",
       "      <td>2020-06-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provenance_id  provider      code version            contact  \\\n",
       "0               1       NPL     EGS++    2016  Ana Denis-Bacelar   \n",
       "1               2   SCK.CEN     MCNPX     2.7      Jérémie Dabin   \n",
       "2               3      IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "3               4      IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "4               5       SGH      GATE     7.2         Erin McKay   \n",
       "5               6       SGH      GATE     7.2         Erin McKay   \n",
       "6               7       SGH      GATE     7.2         Erin McKay   \n",
       "7               8       SGH      GATE     7.2         Erin McKay   \n",
       "8               9       SGH      GATE     7.2         Erin McKay   \n",
       "9              10       SGH      GATE     7.2         Erin McKay   \n",
       "10             11       SGH      GATE     7.2         Erin McKay   \n",
       "11             12       SGH      GATE     7.2         Erin McKay   \n",
       "12             13       SGH      GATE     7.2         Erin McKay   \n",
       "13             14      CRCT      GATE     8.0     Maxime Chauvin   \n",
       "14             15  IEO-CNAO     FLUKA    2011    Francesca Botta   \n",
       "15             16      CRUK  PENELOPE    2014      Nadia Falzone   \n",
       "16             17      CRCT    Geant4    10.5   Alex Vergara Gil   \n",
       "17             18       NPL     EGS++    2016  Ana Denis-Bacelar   \n",
       "18             47      CRUK  PENELOPE    2014      Nadia Falzone   \n",
       "19             48      CRUK  PENELOPE    2014      Nadia Falzone   \n",
       "20             49       NPL     EGS++    2018  Ana Denis-Bacelar   \n",
       "21             50      CRCT    Geant4    10.5     Maxime Chauvin   \n",
       "22             51      IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "23             52      IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "24             53      IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "25             56     PolSl      GATE     8.1       Damian Borys   \n",
       "26             54     PolSl      GATE     8.1       Damian Borys   \n",
       "27             55     PolSl      GATE     8.1       Damian Borys   \n",
       "28             57   SCK.CEN     PHITS    3.10      Jérémie Dabin   \n",
       "\n",
       "                              email       date  \n",
       "0        ana.denisbacelar@npl.co.uk 2017-09-26  \n",
       "1           jeremie.dabin@sckcen.be 2017-10-12  \n",
       "2           aurelie.desbree@irsn.fr 2017-09-28  \n",
       "3           aurelie.desbree@irsn.fr 2018-07-25  \n",
       "4          erin@computerhead.com.au 2018-06-06  \n",
       "5          erin@computerhead.com.au 2018-10-30  \n",
       "6          erin@computerhead.com.au 2018-11-02  \n",
       "7          erin@computerhead.com.au 2018-06-08  \n",
       "8          erin@computerhead.com.au 2018-12-03  \n",
       "9          erin@computerhead.com.au 2018-07-02  \n",
       "10         erin@computerhead.com.au 2018-07-12  \n",
       "11         erin@computerhead.com.au 2018-08-09  \n",
       "12         erin@computerhead.com.au 2018-09-11  \n",
       "13         maxime.chauvin@inserm.fr 2017-08-31  \n",
       "14           francesca.botta@ieo.it 2018-12-04  \n",
       "15  nadia.falzone@oncology.ox.ac.uk 2017-09-08  \n",
       "16       alex.vergara-gil@inserm.fr 2019-04-01  \n",
       "17       ana.denisbacelar@npl.co.uk 2019-04-02  \n",
       "18  nadia.falzone@oncology.ox.ac.uk 2019-05-16  \n",
       "19  nadia.falzone@oncology.ox.ac.uk 2019-08-28  \n",
       "20       ana.denisbacelar@npl.co.uk 2019-10-30  \n",
       "21         maxime.chauvin@inserm.fr 2019-04-01  \n",
       "22          aurelie.desbree@irsn.fr 2020-01-10  \n",
       "23          aurelie.desbree@irsn.fr 2020-01-15  \n",
       "24          aurelie.desbree@irsn.fr 2020-01-14  \n",
       "25            damian.borys@polsl.pl 2020-06-13  \n",
       "26            damian.borys@polsl.pl 2020-06-11  \n",
       "27            damian.borys@polsl.pl 2020-06-12  \n",
       "28          jeremie.dabin@sckcen.be 2020-06-10  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this load all the tables into memory, it's not adapted for large tables like t_safs\n",
    "t_pro = pd.read_sql('t_provenances', con)\n",
    "t_pha = pd.read_sql('t_phantoms', con)\n",
    "t_reg = pd.read_sql('t_regions', con)\n",
    "t_par = pd.read_sql('t_particles', con)\n",
    "\n",
    "pha = {}\n",
    "reg = {}\n",
    "par = {}\n",
    "# build dictionary of region_id for fast queries\n",
    "for i,p in t_pha.iterrows():\n",
    "    pha[p.model] = p.phantom_id\n",
    "    if p.model not in reg: reg[p.model] = {}\n",
    "    regions = t_reg[t_reg.phantom_id==p.phantom_id]\n",
    "    for j,r in regions.iterrows():\n",
    "        reg[p.model][r.region] = {'id': r.region_id, 'mass_g': r.mass_g}\n",
    "for i,p in t_par.iterrows():\n",
    "    par[p['name']] = p.particle_id\n",
    "\n",
    "# t_reg[t_reg.phantom_id==1].to_csv('/home/gate/Downloads/regions_AF.csv',index=False)\n",
    "t_pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read SAF results related to a specific code from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = {'provider':['CRCT'],\n",
    "       'code':['GATE'],\n",
    "       'version':['8.1'],\n",
    "       'contact':['Manuel Bardies'],\n",
    "       'email':['manuel.bardies@inserm.fr']}\n",
    "entry_pro_df = pd.DataFrame(pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from CRCT GATE 8.1 related to OpenDose calcul project with gilles.mathieu@inserm.fr\n",
    "sql  = \"SELECT provenance_id FROM t_provenances WHERE provider='\"+pro['provider'][0]\n",
    "sql += \"' AND code='\"+pro['code'][0]+\"' AND version='\"+pro['version'][0]+\"'\"\n",
    "provenance_id = pd.read_sql(sql, con).provenance_id.values\n",
    "\n",
    "saf_df = pd.DataFrame()\n",
    "for p in provenance_id:\n",
    "    sql = 'SELECT * FROM t_safs WHERE provenance_id='+str(p)\n",
    "    print(sql)\n",
    "    df = pd.read_sql(sql, con)\n",
    "    if saf_df.empty: saf_df = df\n",
    "    else: saf_df = saf_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saf_df.sample(10)\n",
    "saf_df.empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk the directories to find new results and fill the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AF 95 electrons\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-3054baed21a0>\", line 46, in <module>\n",
      "    dbr_data = pd.read_csv(subdir+'/DoseByRegions.txt', sep='\\s+')\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 709, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 449, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 818, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 1049, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 1695, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 565, in pandas._libs.parsers.TextReader.__cinit__\n",
      "pandas.errors.EmptyDataError: No columns to parse from file\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'EmptyDataError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/usr/lib/python3/dist-packages/py/_vendored_packages/apipkg.py\", line 195, in __getattribute__\n",
      "    return getattr(getmod(), name)\n",
      "  File \"/usr/lib/python3/dist-packages/py/_vendored_packages/apipkg.py\", line 179, in getmod\n",
      "    x = importobj(modpath, None)\n",
      "  File \"/usr/lib/python3/dist-packages/py/_vendored_packages/apipkg.py\", line 69, in importobj\n",
      "    module = __import__(modpath, None, None, ['__doc__'])\n",
      "  File \"/usr/lib/python3/dist-packages/pytest.py\", line 13, in <module>\n",
      "    from _pytest.fixtures import fixture, yield_fixture\n",
      "  File \"/usr/lib/python3/dist-packages/_pytest/fixtures.py\", line 842, in <module>\n",
      "    class FixtureFunctionMarker(object):\n",
      "  File \"/usr/lib/python3/dist-packages/_pytest/fixtures.py\", line 844, in FixtureFunctionMarker\n",
      "    params = attr.ib(convert=attr.converters.optional(tuple))\n",
      "TypeError: attrib() got an unexpected keyword argument 'convert'\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# results directory\n",
    "dbr_dir = '/home/gate/data/DBRs/'\n",
    "\n",
    "# loop over results\n",
    "for model in os.listdir(dbr_dir):\n",
    "    for source in os.listdir(dbr_dir+'/'+model):\n",
    "        for particle in os.listdir(dbr_dir+'/'+model+'/'+source):\n",
    "            print(model, source, particle)\n",
    "            # get the corresponding id from the database\n",
    "            source_id = reg[model][int(source)]['id']\n",
    "            particle_id = par[particle]\n",
    "            # make sql query here on source_id and particle_id\n",
    "            if not saf_df.empty:\n",
    "                sp_df = saf_df[(saf_df.source_id == source_id) &\n",
    "                               (saf_df.particle_id == particle_id)]\n",
    "            else:\n",
    "                sp_df = pd.DataFrame()\n",
    "\n",
    "            for subdir, dirs, files in os.walk(dbr_dir+'/'+model+'/'+source+'/'+particle):\n",
    "                if 'DoseByRegions.txt' in files:\n",
    "                    # get model, source, particle, energy, nb_primaries from the directory name\n",
    "                    dirname = subdir.split('/')[-2].split('_')\n",
    "                    energy_MeV = float(dirname[3])\n",
    "                    nb_primaries = int(dirname[4])\n",
    "                    # check if this entry exist in the database\n",
    "                    # (checking this for all targets takes too much time) ?\n",
    "                    if not sp_df.empty:\n",
    "                        sel_df = sp_df[(sp_df.energy_MeV == energy_MeV) &\n",
    "                                       #(sp_df.target_id == target_id) &\n",
    "                                       (sp_df.nb_primaries == nb_primaries)]\n",
    "                    else:\n",
    "                        sel_df = pd.DataFrame()\n",
    "\n",
    "                    # if the entry is not in the database fill it\n",
    "                    if sel_df.empty:\n",
    "                        # get the date from the modification date of the file\n",
    "                        file_time = os.path.getmtime(subdir+'/DoseByRegions.txt')\n",
    "                        entry_pro_df['date'] = datetime.datetime.fromtimestamp(file_time).strftime(\"%Y-%m-%d\")\n",
    "                        # check if this provenance already exist, if not fill the database\n",
    "                        pro_id = df_single_sql_query(entry_pro_df,'provenance_id','t_provenances',con)\n",
    "                        if not pro_id:\n",
    "                            entry_pro_df.to_sql('t_provenances', con, if_exists='append', index=False)\n",
    "                            pro_id = df_single_sql_query(entry_pro_df,'provenance_id','t_provenances',con)\n",
    "\n",
    "                        # read the data for all the targets from DoseByRegions.txt\n",
    "                        dbr_data = pd.read_csv(subdir+'/DoseByRegions.txt', sep='\\s+')\n",
    "                        # create a new df with the entry data to fill the database with df.to_sql()\n",
    "                        new_df = pd.DataFrame(columns=saf_df.columns)\n",
    "                        entry = {}\n",
    "                        for i,row in dbr_data.iterrows():\n",
    "                            target_id = reg[model][int(row['#id'])]['id']\n",
    "                            target_kg = reg[model][int(row['#id'])]['mass_g'] /1000.\n",
    "                            entry['provenance_id'] = pro_id\n",
    "                            entry['source_id'] = source_id\n",
    "                            entry['target_id'] = target_id\n",
    "                            entry['particle_id'] = particle_id\n",
    "                            entry['energy_MeV'] = energy_MeV\n",
    "                            entry['saf'] = row['edep(MeV)']/target_kg/energy_MeV/nb_primaries\n",
    "                            entry['saf_std'] = row['std_edep']*entry['saf']\n",
    "                            entry['nb_primaries'] = nb_primaries\n",
    "                            new_df = new_df.append(pd.DataFrame(entry))\n",
    "                            # print(sel_df[sel_df.target_id == target_id][['saf','saf_std']])\n",
    "                        if not new_df.empty:\n",
    "                            try:\n",
    "                                # fill the database with this entry\n",
    "                                print('... filling the database with',subdir.split('/')[-2])\n",
    "                                #new_df.to_sql('t_safs', con, if_exists='append', index=False)\n",
    "                            except:\n",
    "                                print('Error: the database has refused the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...cleaning NPL EGS++ 2016\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=1\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=18\n",
      "...cleaning SCK.CEN MCNPX 2.7\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=2\n",
      "...cleaning IRSN MCNPX 2.6c\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=3\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=4\n",
      "...cleaning SGH GATE 7.2\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=5\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=6\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=7\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=8\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=9\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=10\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=11\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=12\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=13\n",
      "...cleaning CRCT GATE 8.0\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=14\n",
      "...cleaning IEO-CNAO FLUKA 2011\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=15\n",
      "...cleaning CRUK PENELOPE 2014\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=16\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=47\n",
      "3808 to delete...\n",
      "3808 duplicate rows in t_safs have been deleted.\n",
      "...cleaning CRCT GATE 8.1\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=19\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=20\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=21\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=22\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=23\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=24\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=25\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=26\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=27\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=28\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=29\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=30\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=31\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=32\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=33\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=34\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=35\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=36\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=37\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=38\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=39\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=40\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=41\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=42\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=43\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=44\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=45\n",
      "SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs WHERE provenance_id=46\n"
     ]
    }
   ],
   "source": [
    "# clean by code to be sure there is one entry per source, target, energy\n",
    "t_pro = pd.read_sql('t_provenances', con)\n",
    "t_pro['code_version'] = t_pro['provider'] + ' ' + t_pro['code'] + ' ' + t_pro['version']\n",
    "\n",
    "codes = {}\n",
    "for i,row in t_pro.iterrows():\n",
    "    code = row['code_version']\n",
    "    if code not in codes: \n",
    "        codes[code] = [row['provenance_id']]\n",
    "    else:\n",
    "        codes[code].append(row['provenance_id'])\n",
    "\n",
    "for code,pro_id in codes.items():\n",
    "    # to skip some codes\n",
    "    if (code == 'CRCT Geant4 10.5'): continue\n",
    "    print('...cleaning', code)\n",
    "    saf_df = pd.DataFrame()\n",
    "    for p in pro_id:\n",
    "        sql  = 'SELECT ctid, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs'\n",
    "        sql += ' WHERE provenance_id='+str(p)\n",
    "        print(sql)\n",
    "        df = pd.read_sql(sql, con)\n",
    "        if saf_df.empty: saf_df = df\n",
    "        else: saf_df = saf_df.append(df)\n",
    "    badctid = saf_df[saf_df.duplicated(keep='last',\n",
    "        subset=['source_id', 'target_id', 'particle_id', 'energy_MeV', 'nb_primaries'])].ctid\n",
    "\n",
    "    if not badctid.empty:\n",
    "        # delete the duplicate entries of this code\n",
    "        print(len(badctid),'to delete...')\n",
    "#         sql = 'DELETE FROM t_safs WHERE ctid IN '+str(tuple(badctid))\n",
    "#         result = con.execute(sql)\n",
    "#         print (result.rowcount,'duplicate rows in t_safs have been deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT COUNT(*) FROM t_safs; = 11177347 (2019-05-06)\n",
    "# SELECT COUNT(*) FROM t_safs; = 12608488 (2020-01-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ********************* remove exact duplicates in t_safs *********************\n",
    "# # *****************************************************************************\n",
    "# # delete rows in the list of bad ctid (shorter)\n",
    "# sql = '''\n",
    "# DELETE FROM t_safs a USING \n",
    "# (SELECT max(ctid) AS badctid FROM t_safs GROUP BY t_safs.* HAVING COUNT(*) > 1) b \n",
    "# WHERE a.ctid IN (badctid)\n",
    "# '''\n",
    "# result = con.execute(sql)\n",
    "# print (result.rowcount,'duplicate rows in t_safs have been deleted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to see only a limited number of rows\n",
    "# SELECT * FROM t_safs LIMIT 10;\n",
    "\n",
    "# # check duplicates\n",
    "# SELECT * FROM t_safs GROUP BY t_safs.* HAVING (COUNT(*) > 1);\n",
    "# # check duplicates with different saf or saf_std\n",
    "# SELECT provenance_id, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries FROM t_safs GROUP BY provenance_id, source_id, target_id, particle_id, \"energy_MeV\", nb_primaries HAVING (COUNT(*) > 1);\n",
    "# SELECT max(ctid) AS badctid FROM t_safs GROUP BY t_safs.* HAVING COUNT(*) > 1;\n",
    "\n",
    "# # delete entries\n",
    "# DELETE FROM t_safs WHERE provenance_id=xxx;\n",
    "# DELETE FROM t_provenances WHERE provenance_id=xxx;\n",
    "\n",
    "# # reset the primary key auto increment value after deleting entries\n",
    "# ALTER SEQUENCE t_provenances_provenance_id_seq RESTART WITH xxx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary of results related to a specific code from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # get data from CRCT GATE 8.1 related to OpenDose calcul project with gilles.mathieu@inserm.fr\n",
    "# sql  = \"SELECT provenance_id FROM t_provenances WHERE provider='\"+pro['provider'][0]\n",
    "# sql += \"' AND code='\"+pro['code'][0]+\"' AND version='\"+pro['version'][0]+\"'\"\n",
    "# provenance_id = pd.read_sql(sql, con).provenance_id.values\n",
    "\n",
    "# saf_df = pd.DataFrame()\n",
    "# for p in provenance_id:\n",
    "#     sql = 'SELECT * FROM t_safs WHERE provenance_id='+str(p)\n",
    "#     df = pd.read_sql(sql, con)\n",
    "#     if saf_df.empty:\n",
    "#         print(sql)\n",
    "#         saf_df = df\n",
    "#     else:\n",
    "#         print(sql)\n",
    "#         saf_df = saf_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saf_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time = ~1 min per million rows\n",
    "# db = {'source':{'target':{'particle':{'energy':'nb_primaries'}}}}\n",
    "\n",
    "# for i,row in saf_df.iterrows():\n",
    "#     if row.source_id not in db: \n",
    "#         db[row.source_id] = {}\n",
    "#     if row.target_id not in db[row.source_id]: \n",
    "#         db[row.source_id][row.target_id] = {}\n",
    "#     if row.particle_id not in db[row.source_id][row.target_id]: \n",
    "#         db[row.source_id][row.target_id][row.particle_id] = {}\n",
    "#     db[row.source_id][row.target_id][row.particle_id][row.energy_MeV] = row.nb_primaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/gate/OpenDose/db_GATE8.1.json', 'w') as f:\n",
    "#     json.dump(db, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk the directories to compute SAFs from DoseByRegions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pro = {'provider':['CRCT'],\n",
    "#        'code':['GATE'],\n",
    "#        'version':['8.1'],\n",
    "#        'contact':['Maxime Chauvin'],\n",
    "#        'email':['maxime.chauvin@inserm.fr']}\n",
    "# entry_pro_df = pd.DataFrame(pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # physic constants\n",
    "# Joules_MeV = 1.602176565e-13 # MeV in Joules\n",
    "\n",
    "# # results directory\n",
    "# # dbr_dir = '/home/gate/Downloads/dosebyregions_test/'\n",
    "# dbr_dir = '/home/gate/Downloads/dosebyregions/'\n",
    "\n",
    "# # loop over results\n",
    "# for subdir, dirs, files in os.walk(dbr_dir):\n",
    "#     if 'DoseByRegions.txt' in files:\n",
    "#         # get model, source, particle, energy, nb_primaries from the directory name\n",
    "#         dirname = subdir.split('/')[-2].split('_')\n",
    "#         model, source, particle, energy, nb = dirname[0], dirname[1], dirname[2], dirname[3], dirname[4]\n",
    "#         if particle == 'gamma': particle = 'photons'\n",
    "#         if particle == 'e-': particle = 'electrons'\n",
    "#         print(model, source, particle, energy, nb)\n",
    "#         # get the corresponding id from the database\n",
    "#         phantom_id = pha[model]\n",
    "#         if int(source) not in reg[model]: \n",
    "#             print('caca')\n",
    "#             continue\n",
    "#         source_id = reg[model][int(source)]\n",
    "#         particle_id = par[particle]\n",
    "#         energy_MeV = float(energy)\n",
    "#         nb_primaries = int(nb)\n",
    "#         # print(source_id, particle_id, energy_MeV, nb_primaries)\n",
    "        \n",
    "#         # read the data for all the targets from DoseByRegions.txt\n",
    "#         dbr_data = pd.read_csv(subdir+'/DoseByRegions.txt', sep='\\s+')\n",
    "#         # get the date from the modification date of the file\n",
    "#         file_time = os.path.getmtime(subdir+'/DoseByRegions.txt')\n",
    "#         entry_pro_df['date'] = datetime.datetime.fromtimestamp(file_time).strftime(\"%Y-%m-%d\")\n",
    "#         # check if this provenance already exist, if not fill the database\n",
    "#         pro_id = df_single_sql_query(entry_pro_df,'provenance_id','t_provenances',con)\n",
    "#         if not pro_id:\n",
    "#             entry_pro_df.to_sql('t_provenances', con, if_exists='append', index=False)\n",
    "#             pro_id = df_single_sql_query(entry_pro_df,'provenance_id','t_provenances',con)\n",
    "        \n",
    "#         # check if this entry exist in the database (checking this for all targets takes too much time)\n",
    "#         # build dictionary to make this faster, first make a list of files to add, sorted by source \n",
    "#         # and build dictionary with sql query on this source\n",
    "#         if not saf_df.empty:\n",
    "#             sel_df = saf_df[(saf_df.source_id == source_id) &\n",
    "#                             # (saf_df.target_id == target_id) &\n",
    "#                             (saf_df.particle_id == particle_id) &\n",
    "#                             (saf_df.energy_MeV == energy_MeV) &\n",
    "#                             (saf_df.nb_primaries == nb_primaries)]\n",
    "#         else:\n",
    "#             sel_df = pd.DataFrame()\n",
    "#         # if the entry is not in the database fill it\n",
    "#         if sel_df.empty:\n",
    "#             # create a new df with the entry data to fill the database with df.to_sql()\n",
    "#             new_df = pd.DataFrame(columns=saf_df.columns)\n",
    "#             entry = {}\n",
    "#             for i,row in dbr_data.iterrows():\n",
    "#                 target_id = reg[model][int(row['#id'])]\n",
    "#                 entry['provenance_id'] = pro_id\n",
    "#                 entry['source_id'] = source_id\n",
    "#                 entry['target_id'] = target_id\n",
    "#                 entry['particle_id'] = particle_id\n",
    "#                 entry['energy_MeV'] = energy_MeV\n",
    "#                 entry['saf'] = row['dose(Gy)']/(energy_MeV*Joules_MeV)/nb_primaries\n",
    "#                 entry['saf_std'] = row['std_dose']*entry['saf']\n",
    "#                 entry['nb_primaries'] = nb_primaries\n",
    "#                 new_df = new_df.append(pd.DataFrame(entry))\n",
    "#                 # print(sel_df[sel_df.target_id == target_id][['saf','saf_std']])\n",
    "#             if not new_df.empty:\n",
    "#                 try:\n",
    "#                     # fill the database with this entry\n",
    "#                     print('... filling the database with',subdir.split('/')[-2])\n",
    "#                     new_df.to_sql('t_safs', con, if_exists='append', index=False)\n",
    "#                 except:\n",
    "#                     print('Error: the database has refused the data')\n",
    "\n",
    "# # entry_pro_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
