{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Jupyter Notebook\n",
    "#### Get SAF data from .csv files and upload them to the remote OpenDose database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create fonctions to handle the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(user, password, db, host='localhost'):\n",
    "    '''Returns a connection and a metadata object'''\n",
    "    # We connect with the help of the URL\n",
    "    # postgresql://postgres:postgres@localhost:5432/opendose\n",
    "    url = '{}://{}:{}@{}/{}'\n",
    "    url = url.format('postgresql', user, password, host, db)\n",
    "\n",
    "    # The return value of create_engine() is our connection object\n",
    "    con = sqlalchemy.create_engine(url)#, client_encoding='utf8')\n",
    "    # We then bind the connection to MetaData()\n",
    "    meta = sqlalchemy.MetaData(bind=con) #, reflect=True)\n",
    "    meta.reflect()\n",
    "\n",
    "    return con, meta\n",
    "\n",
    "def df_single_sql_query(df, col_id, table, con):\n",
    "    '''Returns index of a column from a sql query'''\n",
    "    res = []\n",
    "    # allow only single df line query\n",
    "    if len(df.index) != 1:\n",
    "        return None\n",
    "    # compose the sql query from the df dataframe\n",
    "    sql='SELECT '+col_id+' FROM '+table+' WHERE '\n",
    "    for col in df.columns:\n",
    "        if col is not col_id:\n",
    "            sql += col+\"='\"+df[col].iloc[0]+\"' AND \"\n",
    "    sql = sql[:-5]+';'\n",
    "    # execute the sql query on the database table\n",
    "    for line in con.execute(sql):\n",
    "        res.append(line[col_id])\n",
    "    # return the index corresponding to the sql query (if one)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the OpenDose database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(postgresql://postgres:***@localhost/opendose)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read-write user postgres\n",
    "con, meta = connect(user='postgres', password='CRCT_eq15', db='opendose')\n",
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_phantoms\n",
      "t_regions\n",
      "t_particles\n",
      "t_provenances\n",
      "t_safs\n"
     ]
    }
   ],
   "source": [
    "# # # delete all tables\n",
    "# # meta.drop_all(con) # ! Achtung !\n",
    "\n",
    "# t_provenances = sqlalchemy.Table('t_provenances', meta,\n",
    "#     Column('provenance_id', sqlalchemy.Integer, primary_key=True),\n",
    "#     Column('provider', String, nullable=False),\n",
    "#     Column('code', String, nullable=False),\n",
    "#     Column('version', String, nullable=False),\n",
    "#     Column('contact', String, nullable=False),\n",
    "#     Column('email', String, nullable=False),\n",
    "#     Column('date', Date, nullable=False)\n",
    "# )\n",
    "\n",
    "# t_phantoms = sqlalchemy.Table('t_phantoms', meta,\n",
    "#     Column('phantom_id', Integer, primary_key=True),\n",
    "#     Column('provider', String, nullable=False),\n",
    "#     Column('reference', String, nullable=False),\n",
    "#     Column('version', String, nullable=False),\n",
    "#     Column('model', String, nullable=False),\n",
    "#     Column('height', Float, nullable=False),\n",
    "#     Column('mass', Float, nullable=False),\n",
    "#     Column('size_x', Integer, nullable=False),\n",
    "#     Column('size_y', Integer, nullable=False),\n",
    "#     Column('size_z', Integer, nullable=False),\n",
    "#     Column('res_x', Float, nullable=False),\n",
    "#     Column('res_y', Float, nullable=False),\n",
    "#     Column('res_z', Float, nullable=False)\n",
    "# )\n",
    "\n",
    "# t_regions = sqlalchemy.Table('t_regions', meta,\n",
    "#     Column('region_id', Integer, primary_key=True),\n",
    "#     Column('phantom_id', Integer, ForeignKey('t_phantoms.phantom_id'), nullable=False),\n",
    "#     Column('region', Integer, nullable=False),\n",
    "#     Column('name', String, nullable=False),\n",
    "#     Column('volume_cm3', Float, nullable=False),\n",
    "#     Column('mass_g', Float, nullable=False)\n",
    "# )\n",
    "\n",
    "# t_particles = sqlalchemy.Table('t_particles', meta,\n",
    "#     Column('particle_id', Integer, primary_key=True),\n",
    "#     Column('name', String, nullable=False)\n",
    "# )\n",
    "\n",
    "# t_safs = sqlalchemy.Table('t_safs', meta,\n",
    "#     Column('provenance_id', Integer, ForeignKey('t_provenances.provenance_id'), nullable=False),\n",
    "#     Column('source_id', Integer, ForeignKey('t_regions.region_id'), nullable=False),\n",
    "#     Column('target_id', Integer, ForeignKey('t_regions.region_id'), nullable=False),\n",
    "#     Column('particle_id', Integer, ForeignKey('t_particles.particle_id'), nullable=False),\n",
    "#     Column('energy_MeV', Float, nullable=False),\n",
    "#     Column('saf', Float, nullable=False),\n",
    "#     Column('saf_std', Float, nullable=False),\n",
    "#     Column('nb_primaries', Float, nullable=False)\n",
    "# )\n",
    "\n",
    "# # create all tables\n",
    "# meta.create_all(con)\n",
    "\n",
    "for table in meta.tables:\n",
    "    print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provenance_id</th>\n",
       "      <th>provider</th>\n",
       "      <th>code</th>\n",
       "      <th>version</th>\n",
       "      <th>contact</th>\n",
       "      <th>email</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>48</td>\n",
       "      <td>CRUK</td>\n",
       "      <td>PENELOPE</td>\n",
       "      <td>2014</td>\n",
       "      <td>Nadia Falzone</td>\n",
       "      <td>nadia.falzone@oncology.ox.ac.uk</td>\n",
       "      <td>2019-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>49</td>\n",
       "      <td>NPL</td>\n",
       "      <td>EGS++</td>\n",
       "      <td>2018</td>\n",
       "      <td>Ana Denis-Bacelar</td>\n",
       "      <td>ana.denisbacelar@npl.co.uk</td>\n",
       "      <td>2019-10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>CRCT</td>\n",
       "      <td>Geant4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Maxime Chauvin</td>\n",
       "      <td>maxime.chauvin@inserm.fr</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>52</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>53</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>56</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>54</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>55</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>SCK.CEN</td>\n",
       "      <td>PHITS</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Jérémie Dabin</td>\n",
       "      <td>jeremie.dabin@sckcen.be</td>\n",
       "      <td>2020-06-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provenance_id provider      code version            contact  \\\n",
       "19             48     CRUK  PENELOPE    2014      Nadia Falzone   \n",
       "20             49      NPL     EGS++    2018  Ana Denis-Bacelar   \n",
       "21             50     CRCT    Geant4    10.5     Maxime Chauvin   \n",
       "22             51     IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "23             52     IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "24             53     IRSN     MCNPX    2.6c    Aurélie Desbrée   \n",
       "25             56    PolSl      GATE     8.1       Damian Borys   \n",
       "26             54    PolSl      GATE     8.1       Damian Borys   \n",
       "27             55    PolSl      GATE     8.1       Damian Borys   \n",
       "28             57  SCK.CEN     PHITS    3.10      Jérémie Dabin   \n",
       "\n",
       "                              email       date  \n",
       "19  nadia.falzone@oncology.ox.ac.uk 2019-08-28  \n",
       "20       ana.denisbacelar@npl.co.uk 2019-10-30  \n",
       "21         maxime.chauvin@inserm.fr 2019-04-01  \n",
       "22          aurelie.desbree@irsn.fr 2020-01-10  \n",
       "23          aurelie.desbree@irsn.fr 2020-01-15  \n",
       "24          aurelie.desbree@irsn.fr 2020-01-14  \n",
       "25            damian.borys@polsl.pl 2020-06-13  \n",
       "26            damian.borys@polsl.pl 2020-06-11  \n",
       "27            damian.borys@polsl.pl 2020-06-12  \n",
       "28          jeremie.dabin@sckcen.be 2020-06-10  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this load all the tables into memory, it's not adapted for large tables like t_safs\n",
    "# con.execute('SELECT * FROM t_provenances').fetchall()\n",
    "t_pro = pd.read_sql('t_provenances', con)\n",
    "t_pro.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the phantom and particle tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # ICRP phantom table\n",
    "# phantom = {'phantom_id':[1,2],\n",
    "#            'provider':['ICRP','ICRP'],\n",
    "#            'reference':['110','110'],\n",
    "#            'version':['1.2','1.2'], \n",
    "#            'model':['AF','AM'],\n",
    "#            'height':[1.63,1.76],'mass':[60.0,73.0],\n",
    "#            'size_x':[299,254],'size_y':[137,127],'size_z':[348,222],\n",
    "#            'res_x':[1.775,2.137],'res_y':[1.775,2.137],'res_z':[4.84,8.0]}\n",
    "# phantoms = pd.DataFrame(phantom)\n",
    "# phantoms.to_sql('t_phantoms', con, if_exists='append', index=False)\n",
    "\n",
    "# # ICRP regions table\n",
    "# regions_AF = pd.read_table('~/OpenDose/phantoms/ICRP_110_1.2/AF_regions.txt',sep='\\t',engine='python')\n",
    "# regions_AM = pd.read_table('~/OpenDose/phantoms/ICRP_110_1.2/AM_regions.txt',sep='\\t',engine='python')\n",
    "# regions_AF['phantom_id'] = 1\n",
    "# regions_AM['phantom_id'] = 2\n",
    "# regions_AF.to_sql('t_regions', con, if_exists='append', index=False)\n",
    "# regions_AM.to_sql('t_regions', con, if_exists='append', index=False)\n",
    "\n",
    "# # particle table\n",
    "# particle = {'particle_id':[1,2], 'name':['photons','electrons']}\n",
    "# particles = pd.DataFrame(particle)\n",
    "# particles.to_sql('t_particles', con, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert SAF results into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the filename structure is not correct use the command\n",
    "# find . -name \"*.csv\" | while read f; do rename -v 's/AM/AM_/' $f; done\n",
    "# find . -name \"*.csv\" | while read f; do rename -v 's/\\/AF/\\/AF_/' $f; done\n",
    "# find . -name \"*.csv\" | while read f; do rename -v 's/photons/_photons/' $f; done\n",
    "# find . -name \"*.csv\" | while read f; do rename -v 's/electrons/_electrons/' $f; done\n",
    "# sed -i 's|,|.|g' *.csv\n",
    "# sed -i 's|;|,|g' *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file /home/gate/data/SAFs/ICRP_110_1.2/GATE_8.1/CRCT/_toadd/AF_95_electrons_SAF.csv ...\n",
      "processing file /home/gate/data/SAFs/ICRP_110_1.2/GATE_8.1/CRCT/_toadd/AF_95_photons_SAF.csv ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sqlalchemy import *\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# read team information\n",
    "teams = pd.read_csv('contactpersons.csv',names=['id','provider','contact','email'])\n",
    "\n",
    "# number of primaries\n",
    "nb_primaries=1e8\n",
    "\n",
    "# root directory of the SAF results\n",
    "rootdir = '/home/gate/data/SAFs/'\n",
    "\n",
    "# scan the directory tree of the SAF results\n",
    "for root, dirs, files in os.walk(rootdir):\n",
    "    if not dirs:\n",
    "        # ********************* information from the directory structure *********************\n",
    "        # ************************************************************************************\n",
    "        root = root.replace(rootdir,'')\n",
    "        provenance = root.split('/')\n",
    "        # here we have access to the code + version and to the team\n",
    "        provenance_df = teams.loc[teams.provider == provenance[2]].drop('id', axis=1)\n",
    "        provenance_df['code'] = provenance[1].split('_')[0]\n",
    "        provenance_df['version'] = provenance[1].split('_')[1]\n",
    "\n",
    "        # scan the unprocessed .csv file\n",
    "        if provenance[-1] == '_toadd':\n",
    "            for file in files:        \n",
    "                if \"SAF.csv\" in file:\n",
    "                    print ('processing file',rootdir+root+'/'+file,'...')\n",
    "                    # ********************* information from the filename *********************\n",
    "                    # *************************************************************************\n",
    "                    params = file.split('_')\n",
    "                    # get the date from the modification date of the file\n",
    "                    file_time = os.path.getmtime(rootdir+root+'/'+file)\n",
    "                    provenance_df['date'] = datetime.datetime.fromtimestamp(file_time).strftime(\"%Y-%m-%d\")\n",
    "                    # check if this provenance already exist, if not fill the database\n",
    "                    provenance_id = df_single_sql_query(provenance_df,'provenance_id','t_provenances',con)\n",
    "                    if not provenance_id:\n",
    "                        provenance_df.to_sql('t_provenances', con, if_exists='append', index=False)\n",
    "                        provenance_id = df_single_sql_query(provenance_df,'provenance_id','t_provenances',con)\n",
    "                    # here we have access to phantom_id\n",
    "                    phantom = {'provider':[provenance[0].split('_')[0]], 'reference':[provenance[0].split('_')[1]], \n",
    "                               'version':[provenance[0].split('_')[2]], 'model':[params[0]]}\n",
    "                    phantom_df = pd.DataFrame(phantom)\n",
    "                    phantom_id = df_single_sql_query(phantom_df,'phantom_id','t_phantoms',con)\n",
    "                    # here we have access to source_id\n",
    "                    sql = \"SELECT region_id FROM t_regions WHERE phantom_id='\"+str(phantom_id[0])+\"' AND region='\"+params[1]+\"'\"\n",
    "                    source_id = con.execute(sql).fetchall()[0][0]\n",
    "                    # here we have access to particle_id\n",
    "                    sql = \"SELECT particle_id FROM t_particles WHERE name='\"+params[2]+\"'\"\n",
    "                    particle_id = con.execute(sql).fetchall()[0][0]\n",
    "                    # ********************* information from the SAF.csv file *********************\n",
    "                    # *****************************************************************************\n",
    "                    saf_df = pd.read_csv(rootdir+root+'/'+file, index_col=0)\n",
    "                    # here we have access to target_id's\n",
    "                    targets = []\n",
    "                    for col in saf_df.columns:\n",
    "                        target = str(''.join(filter(str.isdigit, col)))\n",
    "#                         print (col,target)\n",
    "                        sql = \"SELECT region_id FROM t_regions WHERE phantom_id='\"+str(phantom_id[0])+\"' AND region='\"+target+\"'\"\n",
    "                        target_id = con.execute(sql).fetchall()\n",
    "                        if target_id:\n",
    "                            targets.append(target_id[0][0])\n",
    "                        else:\n",
    "                            targets.append('None')\n",
    "                    saf_df.columns = targets\n",
    "                    saf_df = saf_df.drop('None', axis=1, errors='ignore')\n",
    "                    # convert the dataframe to have the columns as values in a new column named region_id\n",
    "                    saf_df = saf_df.stack().reset_index(level=1, name='saf').rename(columns={'level_1':'target_id'})\n",
    "                    saf_df.index.names = ['energy_MeV']\n",
    "                    saf_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "                    # ********************* information from the SAFerr.csv file *********************\n",
    "                    # ********************************************************************************\n",
    "                    # do the same to get the saf error per target_id per energy_MeV\n",
    "                    try:\n",
    "                        saferr_df = pd.read_csv(rootdir+root+'/'+file.replace('SAF','SAFerr'), index_col=0)\n",
    "                        # here we have access to target_id's\n",
    "                        targets = []\n",
    "                        for col in saferr_df.columns:\n",
    "                            target = str(''.join(filter(str.isdigit, col)))\n",
    "                            sql = \"SELECT region_id FROM t_regions WHERE phantom_id='\"+str(phantom_id[0])+\"' AND region='\"+target+\"'\"\n",
    "                            target_id = con.execute(sql).fetchall()\n",
    "                            if target_id:\n",
    "                                targets.append(target_id[0][0])\n",
    "                            else:\n",
    "                                targets.append('None')\n",
    "                        saferr_df.columns = targets\n",
    "                        saferr_df = saferr_df.drop('None', axis=1, errors='ignore')\n",
    "                        # convert the dataframe to have the columns as values in a new column named region_id\n",
    "                        saferr_df = saferr_df.stack().reset_index(level=1, name='saf_std').rename(columns={'level_1':'target_id'})\n",
    "                        saferr_df.index.names = ['energy_MeV']\n",
    "                        saferr_df.reset_index(level=0, inplace=True)\n",
    "                        # merge the saferr df with the saf df\n",
    "                        try:\n",
    "                            df_db = pd.merge(saf_df, saferr_df, on=['energy_MeV','target_id'], how='outer', validate=\"one_to_one\")\n",
    "                            # add information to the dataframe for the database\n",
    "                            df_db['provenance_id'] = provenance_id[0]\n",
    "                            df_db['source_id'] = source_id\n",
    "                            df_db['particle_id'] = particle_id\n",
    "                            df_db['nb_primaries'] = nb_primaries\n",
    "                            try:\n",
    "                                # fill the database with SAF and SAFerr\n",
    "                                df_db.to_sql('t_safs', con, if_exists='append', index=False)\n",
    "                                # if everything went well move the files outside the _toadd directory\n",
    "                                shutil.move(rootdir+root+'/'+file,rootdir+root+'/../'+file)\n",
    "                                shutil.move(rootdir+root+'/'+file.replace('SAF','SAFerr'),rootdir+root+'/../'+file.replace('SAF','SAFerr'))\n",
    "                            except:\n",
    "                                print ('Error: the database has refused the data')\n",
    "                                print ('  The SAF.csv or SAFerr.csv may be corrupted (check column contents)')\n",
    "                                print ('  ...moving them to',rootdir+root+'/../_corrupted')\n",
    "                                shutil.move(rootdir+root+'/'+file,rootdir+root+'/../_corrupted/'+file)\n",
    "                                shutil.move(rootdir+root+'/'+file.replace('SAF','SAFerr'),rootdir+root+'/../_corrupted/'+file.replace('SAF','SAFerr'))\n",
    "                        except:\n",
    "                            print ('Error: the SAF.csv and SAFerr.csv cannot be merged:')\n",
    "                            print ('  the rows or columns are not homogeneous')\n",
    "                            print ('  ...moving them to',rootdir+root+'/../_corrupted')\n",
    "                            shutil.move(rootdir+root+'/'+file,rootdir+root+'/../_corrupted/'+file)\n",
    "                            shutil.move(rootdir+root+'/'+file.replace('SAF','SAFerr'),rootdir+root+'/../_corrupted/'+file.replace('SAF','SAFerr'))\n",
    "                    except:\n",
    "                        print ('  Error: no SAFerr.csv file')\n",
    "                        print ('  ...moving the SAF.csv to',rootdir+root+'/../_corrupted')\n",
    "                        shutil.move(rootdir+root+'/'+file,rootdir+root+'/../_corrupted/'+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provenance_id</th>\n",
       "      <th>provider</th>\n",
       "      <th>code</th>\n",
       "      <th>version</th>\n",
       "      <th>contact</th>\n",
       "      <th>email</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>49</td>\n",
       "      <td>NPL</td>\n",
       "      <td>EGS++</td>\n",
       "      <td>2018</td>\n",
       "      <td>Ana Denis-Bacelar</td>\n",
       "      <td>ana.denisbacelar@npl.co.uk</td>\n",
       "      <td>2019-10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>CRCT</td>\n",
       "      <td>Geant4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>Maxime Chauvin</td>\n",
       "      <td>maxime.chauvin@inserm.fr</td>\n",
       "      <td>2019-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>52</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>53</td>\n",
       "      <td>IRSN</td>\n",
       "      <td>MCNPX</td>\n",
       "      <td>2.6c</td>\n",
       "      <td>Aurélie Desbrée</td>\n",
       "      <td>aurelie.desbree@irsn.fr</td>\n",
       "      <td>2020-01-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>56</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>54</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>55</td>\n",
       "      <td>PolSl</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Damian Borys</td>\n",
       "      <td>damian.borys@polsl.pl</td>\n",
       "      <td>2020-06-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>57</td>\n",
       "      <td>SCK.CEN</td>\n",
       "      <td>PHITS</td>\n",
       "      <td>3.10</td>\n",
       "      <td>Jérémie Dabin</td>\n",
       "      <td>jeremie.dabin@sckcen.be</td>\n",
       "      <td>2020-06-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>58</td>\n",
       "      <td>CRCT</td>\n",
       "      <td>GATE</td>\n",
       "      <td>8.1</td>\n",
       "      <td>Manuel Bardies</td>\n",
       "      <td>manuel.bardies@inserm.fr</td>\n",
       "      <td>2020-09-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provenance_id provider    code version            contact                       email       date\n",
       "20             49      NPL   EGS++    2018  Ana Denis-Bacelar  ana.denisbacelar@npl.co.uk 2019-10-30\n",
       "21             50     CRCT  Geant4    10.5     Maxime Chauvin    maxime.chauvin@inserm.fr 2019-04-01\n",
       "22             51     IRSN   MCNPX    2.6c    Aurélie Desbrée     aurelie.desbree@irsn.fr 2020-01-10\n",
       "23             52     IRSN   MCNPX    2.6c    Aurélie Desbrée     aurelie.desbree@irsn.fr 2020-01-15\n",
       "24             53     IRSN   MCNPX    2.6c    Aurélie Desbrée     aurelie.desbree@irsn.fr 2020-01-14\n",
       "25             56    PolSl    GATE     8.1       Damian Borys       damian.borys@polsl.pl 2020-06-13\n",
       "26             54    PolSl    GATE     8.1       Damian Borys       damian.borys@polsl.pl 2020-06-11\n",
       "27             55    PolSl    GATE     8.1       Damian Borys       damian.borys@polsl.pl 2020-06-12\n",
       "28             57  SCK.CEN   PHITS    3.10      Jérémie Dabin     jeremie.dabin@sckcen.be 2020-06-10\n",
       "29             58     CRCT    GATE     8.1     Manuel Bardies    manuel.bardies@inserm.fr 2020-09-24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pro = pd.read_sql('t_provenances', con)\n",
    "t_pro.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provenance_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>target_id</th>\n",
       "      <th>particle_id</th>\n",
       "      <th>energy_MeV</th>\n",
       "      <th>saf</th>\n",
       "      <th>saf_std</th>\n",
       "      <th>nb_primaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.543869e-07</td>\n",
       "      <td>6.705957e-08</td>\n",
       "      <td>100000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>2.866293e-06</td>\n",
       "      <td>1.652519e-06</td>\n",
       "      <td>100000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7.993666e-04</td>\n",
       "      <td>2.860252e-05</td>\n",
       "      <td>100000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>100000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>100000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   provenance_id  source_id  target_id  particle_id  energy_MeV           saf       saf_std  nb_primaries\n",
       "0             58         95          1            2        0.05  1.543869e-07  6.705957e-08   100000000.0\n",
       "1             58         95          2            2        0.05  2.866293e-06  1.652519e-06   100000000.0\n",
       "2             58         95          3            2        0.05  7.993666e-04  2.860252e-05   100000000.0\n",
       "3             58         95          4            2        0.05  0.000000e+00  0.000000e+00   100000000.0\n",
       "4             58         95          5            2        0.05  0.000000e+00  0.000000e+00   100000000.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_saf = pd.read_sql('SELECT * FROM t_safs WHERE provenance_id=58', con)\n",
    "t_saf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT COUNT(*) FROM t_safs; = 11177347 (2019-05-06)\n",
    "# SELECT COUNT(*) FROM t_safs; = 12608488 (2020-01-06)\n",
    "# SELECT COUNT(*) FROM t_safs; = 12624553 (2020-01-10)\n",
    "# SELECT COUNT(*) FROM t_safs; = 12633905 (2020-01-15)\n",
    "# SELECT COUNT(*) FROM t_safs; = 19604106 (2020-02-25)\n",
    "# SELECT COUNT(*) FROM t_safs; = 19608810 (2020-06-23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## these are for psql. TODO: write a generic SQL script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import *\n",
    "\n",
    "print ('Cleaning the database...')\n",
    "\n",
    "# # ********************* remove loose entries in t_provenances *********************\n",
    "# # *********************************************************************************\n",
    "# sql = '''\n",
    "# DELETE FROM t_provenances a \n",
    "# WHERE NOT EXISTS \n",
    "# (SELECT * FROM t_safs b WHERE a.provenance_id = b.provenance_id)\n",
    "# '''\n",
    "# result = con.execute(sql)\n",
    "# print (result.rowcount,'orphen provenances in t_provenances have been deleted.')\n",
    "\n",
    "# # ********************* remove duplicates in t_safs *********************\n",
    "# # ***********************************************************************\n",
    "# # delete rows in the list of bad ctid (shorter)\n",
    "# sql = '''\n",
    "# DELETE FROM t_safs a USING \n",
    "# (SELECT max(ctid) AS badctid FROM t_safs GROUP BY t_safs.* HAVING COUNT(*) > 1) b \n",
    "# WHERE a.ctid IN (badctid)\n",
    "# '''\n",
    "# result = con.execute(sql)\n",
    "# print (result.rowcount,'duplicate rows in t_safs have been deleted.')\n",
    "# 2019-03-25 there are 2067821 entries in the database\n",
    "# 2019-04-04 there are 7812678 entries in the database\n",
    "\n",
    "# # clean by code to be sure there is one entry per source, target, energy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to see only a limited number of rows\n",
    "# SELECT * FROM t_safs LIMIT 10;\n",
    "\n",
    "# # check duplicates\n",
    "# SELECT * FROM t_safs GROUP BY t_safs.* HAVING (COUNT(*) > 1);\n",
    "# # check duplicates with different saf or saf_std\n",
    "# SELECT provenance_id, source_id, target_id, particle_id, energy_MeV, nb_primaries FROM t_safs GROUP BY provenance_id, source_id, target_id, particle_id, energy_MeV, nb_primaries HAVING (COUNT(*) > 1);\n",
    "\n",
    "# # delete entries\n",
    "# DELETE FROM t_safs WHERE provenance_id=xxx;\n",
    "# DELETE FROM t_provenances WHERE provenance_id=xxx;\n",
    "\n",
    "# # reset the primary key auto increment value after deleting entries\n",
    "# ALTER SEQUENCE t_provenances_provenance_id_seq RESTART WITH xxx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
